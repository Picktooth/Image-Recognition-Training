{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c494733f-00e6-4d91-aeeb-8f4714a6ca34",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "# Basics\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets\n",
    "import numpy as np\n",
    "import cv2\n",
    "# Adding noise\n",
    "from skimage.util import random_noise\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# For CNN\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# For Loss Func. And Optimization\n",
    "import torch.optim as optim\n",
    "\n",
    "# For Visual Representation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# In case defined class doesn't work, create new dataset from this\n",
    "from torchvision.datasets import ImageFolder as img_fold\n",
    "\n",
    "print(\"Success\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a8ce311e",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "TRAIN_NOISE = True\n",
    "TEST_NOISE = False\n",
    "BATCH_SIZE = 4\n",
    "NUM_CLASSES = 10\n",
    "GENERATIONS = 100\n",
    "pretrained=True\n",
    "requires_grad=True\n",
    "\n",
    "transformer = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee8a0a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89a8a5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Data Downloading\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transformer)\n",
    "\n",
    "# Data Loader\n",
    "trainLoader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "# Image Categories\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cb7f27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTIONS\n",
    "\n",
    "def gaussian_noise():\n",
    "    for data in trainLoader:\n",
    "        img, labels = data[0], data[1]\n",
    "        gauss_img = torch.tensor(random_noise(img, mode='gaussian', mean=0, var=0.05, clip=True))\n",
    "        \n",
    "        torchvision.utils.save_image(img, fr\"C:\\Users\\John\\Documents\\Visual Studio Code Python\\Image_Classification_Training\\CIFAR10 Gaussian Noise\\Img Without Noise\\{i}_normal.png\")\n",
    "        torchvision.utils.save_image(gauss_img, fr\"C:\\Users\\John\\Documents\\Visual Studio Code Python\\Image_Classification_Training\\CIFAR10 Gaussian Noise\\Img With Noise\\{i}_gaussian.png\")\n",
    "        \n",
    "        # Grabbing image labels (Truth)\n",
    "        # print(' '.join(f'{classes[labels[j]]:5s}' for j in range(BATCH_SIZE)))\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "673ab5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating new images with gaussian noise\n",
    "for i in range(GENERATIONS):\n",
    "    gaussian_noise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "094c4a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting root directory for training data\n",
    "trainroot = \"./Noiseimg/train\"\n",
    "testroot = \"./Noiseimg/test\"\n",
    "# Using ImageFolder to designate and transform our entire dataset (determine if necessary to transform here and in dataloaders)\n",
    "trainset = img_fold(root=trainroot, transform=transformer)\n",
    "testset = img_fold(root=testroot, transform=transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "255ffe08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels:  ['bird', 'car', 'cat', 'deer', 'dog', 'frog', 'horse', 'plane', 'ship', 'truck']\n",
      "Trainset Length:  100\n",
      "Testset Length:  23\n"
     ]
    }
   ],
   "source": [
    "# grabbing labels\n",
    "class_names = trainset.classes\n",
    "print('Labels: ',class_names)\n",
    "print('Trainset Length: ', len(trainset))\n",
    "print('Testset Length: ', len(testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3f39d0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image tensor:\n",
      "tensor([[[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "         ...,\n",
      "         [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "         [-0.8353, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]],\n",
      "\n",
      "        [[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "         ...,\n",
      "         [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "         [-0.8353, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]],\n",
      "\n",
      "        [[-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "         ...,\n",
      "         [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000],\n",
      "         [-1.0000, -1.0000, -1.0000,  ..., -1.0000, -1.0000, -1.0000]]])\n",
      "Image shape: torch.Size([3, 32, 32])\n",
      "Image datatype: torch.float32\n",
      "Image label: 0\n",
      "Label datatype: <class 'int'>\n"
     ]
    }
   ],
   "source": [
    "# Storing tensors for images and labels\n",
    "\n",
    "img, label = trainset[0][0], trainset[0][1]\n",
    "print(f\"Image tensor:\\n{img}\")\n",
    "print(f\"Image shape: {img.shape}\")\n",
    "print(f\"Image datatype: {img.dtype}\")\n",
    "print(f\"Image label: {label}\")\n",
    "print(f\"Label datatype: {type(label)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454f4073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset created, proceed training like normal\n",
    "# Consider creating a way to check number of pictures in a folder; designate number of photos per class folder -> fill each class folder until number is met | Consider using the length function to get the length of the folder\n",
    "# Modify to use as a general template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f1d4ca44",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
    "                                            shuffle=True, num_workers=1)\n",
    "\n",
    "testloader =torch.utils.data.DataLoader(testset, batch_size = BATCH_SIZE,\n",
    "                                            shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "4384fbac",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'cuda'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[90], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39m# Randomly selected training img\u001b[39;00m\n\u001b[0;32m      9\u001b[0m dataiter \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(trainloader)\n\u001b[1;32m---> 10\u001b[0m images, labels \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(dataiter)\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m     12\u001b[0m \u001b[39m# show images\u001b[39;00m\n\u001b[0;32m     13\u001b[0m imshow(torchvision\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mmake_grid(images))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'cuda'"
     ]
    }
   ],
   "source": [
    "# Show image\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # Unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "    \n",
    "# Randomly selected training img\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "# print labels\n",
    "print(' '.join(f'{trainset.classes[labels[j]]:5s}' for j in range(BATCH_SIZE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "06579f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([3, 32, 32]) -> [batch_size, color_channels, height, width]\n",
      "Label shape: torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Image shape: {img.shape} -> [batch_size, color_channels, height, width]\")\n",
    "print(f\"Label shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9eca5fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    # Defining a network\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 10, 5) #Change argument 2 to experiment, must match argument 1 of conv 2\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(10, 16, 5) #Change argument 1 to experiment, must match argument 2 of conv 1\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    # Defining forward pass\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "96e6e201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up loss func. and optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr = 0.001, momentum = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9dd8074a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available\n",
      "\n",
      "Train loss:  2.290065288543701   Train Acc:  0.0\n",
      "Train loss:  4.542020082473755   Train Acc:  0.25\n",
      "Train loss:  6.866601943969727   Train Acc:  0.25\n",
      "Train loss:  9.207083702087402   Train Acc:  0.25\n",
      "Train loss:  11.48572039604187   Train Acc:  0.5\n",
      "Train loss:  13.82283329963684   Train Acc:  0.5\n",
      "Train loss:  16.1131911277771   Train Acc:  0.5\n",
      "Train loss:  18.415929317474365   Train Acc:  0.5\n",
      "Train loss:  20.720757484436035   Train Acc:  0.5\n",
      "Train loss:  23.055957317352295   Train Acc:  0.5\n",
      "Train loss:  25.329378128051758   Train Acc:  0.75\n",
      "Train loss:  27.62749481201172   Train Acc:  1.0\n",
      "Train loss:  29.91543960571289   Train Acc:  1.25\n",
      "Train loss:  32.17081594467163   Train Acc:  1.5\n",
      "Train loss:  34.488791942596436   Train Acc:  1.5\n",
      "Train loss:  36.839149475097656   Train Acc:  1.5\n",
      "Train loss:  39.15896272659302   Train Acc:  1.75\n",
      "Train loss:  41.48889946937561   Train Acc:  1.75\n",
      "Train loss:  43.760838747024536   Train Acc:  1.75\n",
      "Train loss:  46.01416611671448   Train Acc:  2.0\n",
      "Train loss:  48.29066276550293   Train Acc:  2.25\n",
      "Train loss:  50.64082670211792   Train Acc:  2.25\n",
      "Train loss:  52.89590549468994   Train Acc:  2.5\n",
      "Train loss:  55.19306945800781   Train Acc:  2.5\n",
      "Train loss:  57.481608390808105   Train Acc:  2.75\n",
      "Train loss:  59.77472138404846   Train Acc:  3.0\n",
      "Train loss:  62.109036684036255   Train Acc:  3.0\n",
      "Train loss:  64.40823483467102   Train Acc:  3.0\n",
      "Train loss:  66.69258618354797   Train Acc:  3.0\n",
      "Train loss:  69.01519346237183   Train Acc:  3.0\n",
      "Train loss:  71.24108839035034   Train Acc:  3.0\n",
      "Train loss:  73.50622081756592   Train Acc:  3.25\n",
      "Train loss:  75.80222129821777   Train Acc:  3.5\n",
      "Train loss:  78.10294389724731   Train Acc:  3.5\n",
      "Train loss:  80.41616582870483   Train Acc:  3.75\n",
      "Train loss:  82.73119759559631   Train Acc:  3.75\n",
      "Train loss:  85.00325036048889   Train Acc:  3.75\n",
      "Train loss:  87.36863970756531   Train Acc:  3.75\n",
      "Train loss:  89.70851135253906   Train Acc:  3.75\n",
      "Train loss:  91.96851944923401   Train Acc:  4.0\n",
      "Train loss:  94.25173735618591   Train Acc:  4.25\n",
      "Train loss:  96.56134700775146   Train Acc:  4.25\n",
      "Train loss:  98.78196096420288   Train Acc:  4.75\n",
      "Train loss:  101.12855839729309   Train Acc:  4.75\n",
      "Train loss:  103.37290501594543   Train Acc:  5.25\n",
      "Train loss:  105.66044688224792   Train Acc:  5.25\n",
      "Train loss:  107.93349266052246   Train Acc:  5.5\n",
      "Train loss:  110.239426612854   Train Acc:  5.5\n",
      "Train loss:  112.5443058013916   Train Acc:  5.5\n",
      "Train loss:  114.86542320251465   Train Acc:  5.5\n",
      "Train loss:  117.15255808830261   Train Acc:  5.5\n",
      "Train loss:  119.4139723777771   Train Acc:  5.5\n",
      "Train loss:  121.7190477848053   Train Acc:  5.75\n",
      "Train loss:  123.98056364059448   Train Acc:  6.0\n",
      "Train loss:  126.29132843017578   Train Acc:  6.0\n",
      "Train loss:  128.62267780303955   Train Acc:  6.0\n",
      "Train loss:  130.93606972694397   Train Acc:  6.0\n",
      "Train loss:  133.17508482933044   Train Acc:  6.25\n",
      "Train loss:  135.53250575065613   Train Acc:  6.25\n",
      "Train loss:  137.83881950378418   Train Acc:  6.25\n",
      "Train loss:  140.12673664093018   Train Acc:  6.25\n",
      "Train loss:  142.43696570396423   Train Acc:  6.25\n",
      "Train loss:  144.7058663368225   Train Acc:  6.5\n",
      "Train loss:  147.030837059021   Train Acc:  6.5\n",
      "Train loss:  149.29702425003052   Train Acc:  6.75\n",
      "Train loss:  151.56946873664856   Train Acc:  7.0\n",
      "Train loss:  153.88399577140808   Train Acc:  7.0\n",
      "Train loss:  156.18185257911682   Train Acc:  7.0\n",
      "Train loss:  158.42396473884583   Train Acc:  7.25\n",
      "Train loss:  160.74130034446716   Train Acc:  7.25\n",
      "Train loss:  163.00628280639648   Train Acc:  7.25\n",
      "Train loss:  165.24070644378662   Train Acc:  7.75\n",
      "Train loss:  167.55249190330505   Train Acc:  7.75\n",
      "Train loss:  169.85964727401733   Train Acc:  8.0\n",
      "Train loss:  172.1774034500122   Train Acc:  8.25\n",
      "Train loss:  174.4357829093933   Train Acc:  8.5\n",
      "Train loss:  176.72614312171936   Train Acc:  8.75\n",
      "Train loss:  178.97311329841614   Train Acc:  9.25\n",
      "Train loss:  181.23301196098328   Train Acc:  9.25\n",
      "Train loss:  183.54455280303955   Train Acc:  9.25\n",
      "Train loss:  185.77673840522766   Train Acc:  9.75\n",
      "Train loss:  188.095552444458   Train Acc:  9.75\n",
      "Train loss:  190.39746069908142   Train Acc:  9.75\n",
      "Train loss:  192.6014564037323   Train Acc:  10.25\n",
      "Train loss:  194.9017608165741   Train Acc:  10.25\n",
      "Train loss:  197.2081425189972   Train Acc:  10.25\n",
      "Train loss:  199.4975438117981   Train Acc:  10.25\n",
      "Train loss:  201.87364268302917   Train Acc:  10.25\n",
      "Train loss:  204.1156768798828   Train Acc:  10.25\n",
      "Train loss:  206.40722632408142   Train Acc:  10.25\n",
      "Train loss:  208.71706199645996   Train Acc:  10.25\n",
      "Train loss:  210.99003672599792   Train Acc:  10.25\n",
      "Train loss:  213.3069441318512   Train Acc:  10.25\n",
      "Train loss:  215.6691915988922   Train Acc:  10.25\n",
      "Train loss:  217.91527938842773   Train Acc:  10.25\n",
      "Train loss:  220.24905490875244   Train Acc:  10.25\n",
      "Train loss:  222.44212007522583   Train Acc:  11.0\n",
      "Train loss:  224.80830359458923   Train Acc:  11.0\n",
      "Train loss:  227.08763194084167   Train Acc:  11.0\n",
      "Train loss:  229.41066932678223   Train Acc:  11.0\n",
      "Train loss:  231.74596691131592   Train Acc:  11.0\n",
      "Train loss:  233.94688510894775   Train Acc:  11.25\n",
      "Train loss:  236.17841792106628   Train Acc:  11.5\n",
      "Train loss:  238.47993350028992   Train Acc:  11.5\n",
      "Train loss:  240.8107726573944   Train Acc:  11.5\n",
      "Train loss:  243.13616108894348   Train Acc:  11.5\n",
      "Train loss:  245.50229454040527   Train Acc:  11.5\n",
      "Train loss:  247.78398942947388   Train Acc:  11.5\n",
      "Train loss:  250.08890223503113   Train Acc:  11.75\n",
      "Train loss:  252.43346190452576   Train Acc:  11.75\n",
      "Train loss:  254.75499320030212   Train Acc:  11.75\n",
      "Train loss:  257.01802706718445   Train Acc:  11.75\n",
      "Train loss:  259.3010540008545   Train Acc:  11.75\n",
      "Train loss:  261.58291387557983   Train Acc:  11.75\n",
      "Train loss:  263.80088472366333   Train Acc:  12.0\n",
      "Train loss:  266.11832213401794   Train Acc:  12.0\n",
      "Train loss:  268.33954977989197   Train Acc:  12.25\n",
      "Train loss:  270.5730426311493   Train Acc:  12.5\n",
      "Train loss:  272.89662766456604   Train Acc:  12.75\n",
      "Train loss:  275.115257024765   Train Acc:  13.0\n",
      "Train loss:  277.3902931213379   Train Acc:  13.5\n",
      "Train loss:  279.71145486831665   Train Acc:  13.5\n",
      "Train loss:  282.0498127937317   Train Acc:  13.5\n",
      "Train loss:  284.2824635505676   Train Acc:  13.75\n",
      "Train loss:  286.57355785369873   Train Acc:  14.0\n",
      "Train loss:  288.87085461616516   Train Acc:  14.0\n",
      "Train loss:  291.1388940811157   Train Acc:  14.0\n",
      "Train loss:  293.43857502937317   Train Acc:  14.25\n",
      "Train loss:  295.658851146698   Train Acc:  14.75\n",
      "Train loss:  297.92755246162415   Train Acc:  14.75\n",
      "Train loss:  300.25304794311523   Train Acc:  14.75\n",
      "Train loss:  302.56450963020325   Train Acc:  14.75\n",
      "Train loss:  304.88868951797485   Train Acc:  14.75\n",
      "Train loss:  307.19755840301514   Train Acc:  14.75\n",
      "Train loss:  309.5450382232666   Train Acc:  14.75\n",
      "Train loss:  311.82604694366455   Train Acc:  15.0\n",
      "Train loss:  314.2002456188202   Train Acc:  15.0\n",
      "Train loss:  316.50871205329895   Train Acc:  15.0\n",
      "Train loss:  318.7043101787567   Train Acc:  15.25\n",
      "Train loss:  321.0337452888489   Train Acc:  15.5\n",
      "Train loss:  323.38496112823486   Train Acc:  15.5\n",
      "Train loss:  325.6955819129944   Train Acc:  15.5\n",
      "Train loss:  327.93006896972656   Train Acc:  15.5\n",
      "Train loss:  330.1281361579895   Train Acc:  16.25\n",
      "Train loss:  332.3592302799225   Train Acc:  16.5\n",
      "Train loss:  334.5780529975891   Train Acc:  16.75\n",
      "Train loss:  336.87054443359375   Train Acc:  17.0\n",
      "Train loss:  339.1765275001526   Train Acc:  17.25\n",
      "Train loss:  341.4406929016113   Train Acc:  17.25\n",
      "Train loss:  343.67914724349976   Train Acc:  17.75\n",
      "Train loss:  345.93774819374084   Train Acc:  18.25\n",
      "Train loss:  348.2441623210907   Train Acc:  18.25\n",
      "Train loss:  350.54438066482544   Train Acc:  18.25\n",
      "Train loss:  352.8220796585083   Train Acc:  18.5\n",
      "Train loss:  355.09544563293457   Train Acc:  18.5\n",
      "Train loss:  357.35016775131226   Train Acc:  19.0\n",
      "Train loss:  359.68720388412476   Train Acc:  19.0\n",
      "Train loss:  361.94937777519226   Train Acc:  19.0\n",
      "Train loss:  364.214492559433   Train Acc:  19.25\n",
      "Train loss:  366.55335688591003   Train Acc:  19.25\n",
      "Train loss:  368.87734055519104   Train Acc:  19.25\n",
      "Train loss:  371.14173793792725   Train Acc:  19.25\n",
      "Train loss:  373.3959856033325   Train Acc:  19.5\n",
      "Train loss:  375.66031765937805   Train Acc:  19.5\n",
      "Train loss:  377.9123594760895   Train Acc:  19.75\n",
      "Train loss:  380.1550524234772   Train Acc:  19.75\n",
      "Train loss:  382.4725625514984   Train Acc:  19.75\n",
      "Train loss:  384.7014174461365   Train Acc:  20.25\n",
      "Train loss:  386.99811267852783   Train Acc:  20.25\n",
      "Train loss:  389.239723443985   Train Acc:  20.5\n",
      "Train loss:  391.55759596824646   Train Acc:  20.5\n",
      "Train loss:  393.89027643203735   Train Acc:  20.5\n",
      "Train loss:  396.1140751838684   Train Acc:  20.75\n",
      "Train loss:  398.4189438819885   Train Acc:  21.0\n",
      "Train loss:  400.7309150695801   Train Acc:  21.25\n",
      "Train loss:  403.0665183067322   Train Acc:  21.25\n",
      "Train loss:  405.34507632255554   Train Acc:  21.25\n",
      "Train loss:  407.63666319847107   Train Acc:  21.25\n",
      "Train loss:  409.95402550697327   Train Acc:  21.25\n",
      "Train loss:  412.1265504360199   Train Acc:  22.0\n",
      "Train loss:  414.43555545806885   Train Acc:  22.25\n",
      "Train loss:  416.78931856155396   Train Acc:  22.25\n",
      "Train loss:  419.14870858192444   Train Acc:  22.25\n",
      "Train loss:  421.42349076271057   Train Acc:  22.25\n",
      "Train loss:  423.6790041923523   Train Acc:  22.5\n",
      "Train loss:  426.00914669036865   Train Acc:  22.5\n",
      "Train loss:  428.33707666397095   Train Acc:  22.5\n",
      "Train loss:  430.5872235298157   Train Acc:  23.0\n",
      "Train loss:  432.8036313056946   Train Acc:  23.25\n",
      "Train loss:  435.07879543304443   Train Acc:  23.25\n",
      "Train loss:  437.3170163631439   Train Acc:  23.5\n",
      "Train loss:  439.61468029022217   Train Acc:  23.75\n",
      "Train loss:  441.8892741203308   Train Acc:  23.75\n",
      "Train loss:  444.17726469039917   Train Acc:  24.0\n",
      "Train loss:  446.44249844551086   Train Acc:  24.25\n",
      "Train loss:  448.7659754753113   Train Acc:  24.5\n",
      "Train loss:  451.0198519229889   Train Acc:  24.5\n",
      "Train loss:  453.20602798461914   Train Acc:  25.0\n",
      "Train loss:  455.41958475112915   Train Acc:  25.25\n",
      "Train loss:  457.6733214855194   Train Acc:  25.5\n",
      "Train loss:  459.93792700767517   Train Acc:  26.0\n",
      "Train loss:  462.1411452293396   Train Acc:  26.5\n",
      "Train loss:  464.4353311061859   Train Acc:  26.75\n",
      "Train loss:  466.7634835243225   Train Acc:  27.0\n",
      "Train loss:  469.0165185928345   Train Acc:  27.0\n",
      "Train loss:  471.3204894065857   Train Acc:  27.0\n",
      "Train loss:  473.58796882629395   Train Acc:  27.0\n",
      "Train loss:  475.82153272628784   Train Acc:  27.0\n",
      "Train loss:  478.02409410476685   Train Acc:  27.25\n",
      "Train loss:  480.2841501235962   Train Acc:  27.5\n",
      "Train loss:  482.5575044155121   Train Acc:  27.75\n",
      "Train loss:  484.84534215927124   Train Acc:  27.75\n",
      "Train loss:  486.9947156906128   Train Acc:  28.25\n",
      "Train loss:  489.32208609580994   Train Acc:  28.25\n",
      "Train loss:  491.67167711257935   Train Acc:  28.25\n",
      "Train loss:  493.8565549850464   Train Acc:  28.75\n",
      "Train loss:  496.16948795318604   Train Acc:  28.75\n",
      "Train loss:  498.497022151947   Train Acc:  28.75\n",
      "Train loss:  500.7612352371216   Train Acc:  29.0\n",
      "Train loss:  503.1301920413971   Train Acc:  29.0\n",
      "Train loss:  505.3641691207886   Train Acc:  29.0\n",
      "Train loss:  507.63220953941345   Train Acc:  29.0\n",
      "Train loss:  510.01253867149353   Train Acc:  29.0\n",
      "Train loss:  512.3806114196777   Train Acc:  29.0\n",
      "Train loss:  514.5666832923889   Train Acc:  29.25\n",
      "Train loss:  516.7880687713623   Train Acc:  29.25\n",
      "Train loss:  519.0912322998047   Train Acc:  29.25\n",
      "Train loss:  521.393837928772   Train Acc:  29.25\n",
      "Train loss:  523.5926899909973   Train Acc:  29.75\n",
      "Train loss:  525.748592376709   Train Acc:  30.25\n",
      "Train loss:  528.015371799469   Train Acc:  30.5\n",
      "Train loss:  530.3647615909576   Train Acc:  30.5\n",
      "Train loss:  532.6702542304993   Train Acc:  30.5\n",
      "Train loss:  534.9332571029663   Train Acc:  30.75\n",
      "Train loss:  537.1999316215515   Train Acc:  30.75\n",
      "Train loss:  539.5531718730927   Train Acc:  30.75\n",
      "Train loss:  541.8269891738892   Train Acc:  31.0\n",
      "Train loss:  543.9713733196259   Train Acc:  31.25\n",
      "Train loss:  546.2848062515259   Train Acc:  31.25\n",
      "Train loss:  548.6219639778137   Train Acc:  31.25\n",
      "Train loss:  550.9013652801514   Train Acc:  31.25\n",
      "Train loss:  553.2480211257935   Train Acc:  31.25\n",
      "Train loss:  555.5040447711945   Train Acc:  31.5\n",
      "Train loss:  557.7450201511383   Train Acc:  32.0\n",
      "Train loss:  560.0227584838867   Train Acc:  32.25\n",
      "Train loss:  562.299188375473   Train Acc:  32.25\n",
      "Train loss:  564.5642538070679   Train Acc:  32.5\n",
      "Train loss:  566.8349027633667   Train Acc:  32.75\n",
      "Train loss:  569.0824871063232   Train Acc:  33.0\n",
      "Train loss:  571.3870129585266   Train Acc:  33.0\n",
      "Final Train loss:  22.855480518341064   Final Train Acc:  1.32\n"
     ]
    }
   ],
   "source": [
    "# Setting up CUDA Device\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Available\")\n",
    "    device = torch.device(\"cuda\")\n",
    "    net.cuda()\n",
    "    print(\"\")\n",
    "    \n",
    "# Setup train loss and train accuracy values\n",
    "train_loss, train_acc = 0, 0\n",
    "    \n",
    "# Loop through data loader data batches\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch, (X, y) in enumerate(trainloader):\n",
    "        # Send data to target device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        y_pred = net(X)\n",
    "\n",
    "        # 2. Calculate  and accumulate loss\n",
    "        loss = criterion(y_pred, y)\n",
    "        train_loss += loss.item() \n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate and accumulate accuracy metric across all batches\n",
    "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
    "        print('Train loss: ', train_loss,' ', 'Train Acc: ',train_acc)\n",
    "\n",
    "# Adjust metrics to get average loss and accuracy per batch \n",
    "train_loss = train_loss / len(trainloader)\n",
    "train_acc = train_acc / len(trainloader)\n",
    "print('Final Train loss: ', train_loss,' ', 'Final Train Acc: ',train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3b702f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available\n",
      "\n",
      "[1,     1] loss: 0.001\n",
      "[1,     2] loss: 0.001\n",
      "[1,     3] loss: 0.001\n",
      "[1,     4] loss: 0.001\n",
      "[1,     5] loss: 0.001\n",
      "[1,     6] loss: 0.001\n",
      "[1,     7] loss: 0.001\n",
      "[1,     8] loss: 0.001\n",
      "[1,     9] loss: 0.001\n",
      "[1,    10] loss: 0.001\n",
      "[1,    11] loss: 0.001\n",
      "[1,    12] loss: 0.001\n",
      "[1,    13] loss: 0.001\n",
      "[1,    14] loss: 0.001\n",
      "[1,    15] loss: 0.001\n",
      "[1,    16] loss: 0.001\n",
      "[1,    17] loss: 0.001\n",
      "[1,    18] loss: 0.001\n",
      "[1,    19] loss: 0.001\n",
      "[1,    20] loss: 0.001\n",
      "[1,    21] loss: 0.001\n",
      "[1,    22] loss: 0.001\n",
      "[1,    23] loss: 0.001\n",
      "[1,    24] loss: 0.001\n",
      "[1,    25] loss: 0.001\n",
      "[2,     1] loss: 0.001\n",
      "[2,     2] loss: 0.001\n",
      "[2,     3] loss: 0.001\n",
      "[2,     4] loss: 0.001\n",
      "[2,     5] loss: 0.001\n",
      "[2,     6] loss: 0.001\n",
      "[2,     7] loss: 0.001\n",
      "[2,     8] loss: 0.001\n",
      "[2,     9] loss: 0.001\n",
      "[2,    10] loss: 0.001\n",
      "[2,    11] loss: 0.001\n",
      "[2,    12] loss: 0.001\n",
      "[2,    13] loss: 0.001\n",
      "[2,    14] loss: 0.001\n",
      "[2,    15] loss: 0.001\n",
      "[2,    16] loss: 0.001\n",
      "[2,    17] loss: 0.001\n",
      "[2,    18] loss: 0.001\n",
      "[2,    19] loss: 0.001\n",
      "[2,    20] loss: 0.001\n",
      "[2,    21] loss: 0.001\n",
      "[2,    22] loss: 0.001\n",
      "[2,    23] loss: 0.001\n",
      "[2,    24] loss: 0.001\n",
      "[2,    25] loss: 0.001\n",
      "[3,     1] loss: 0.001\n",
      "[3,     2] loss: 0.001\n",
      "[3,     3] loss: 0.001\n",
      "[3,     4] loss: 0.001\n",
      "[3,     5] loss: 0.001\n",
      "[3,     6] loss: 0.001\n",
      "[3,     7] loss: 0.001\n",
      "[3,     8] loss: 0.001\n",
      "[3,     9] loss: 0.001\n",
      "[3,    10] loss: 0.001\n",
      "[3,    11] loss: 0.001\n",
      "[3,    12] loss: 0.001\n",
      "[3,    13] loss: 0.001\n",
      "[3,    14] loss: 0.001\n",
      "[3,    15] loss: 0.001\n",
      "[3,    16] loss: 0.001\n",
      "[3,    17] loss: 0.001\n",
      "[3,    18] loss: 0.001\n",
      "[3,    19] loss: 0.001\n",
      "[3,    20] loss: 0.001\n",
      "[3,    21] loss: 0.001\n",
      "[3,    22] loss: 0.001\n",
      "[3,    23] loss: 0.001\n",
      "[3,    24] loss: 0.001\n",
      "[3,    25] loss: 0.001\n",
      "[4,     1] loss: 0.001\n",
      "[4,     2] loss: 0.001\n",
      "[4,     3] loss: 0.001\n",
      "[4,     4] loss: 0.001\n",
      "[4,     5] loss: 0.001\n",
      "[4,     6] loss: 0.001\n",
      "[4,     7] loss: 0.001\n",
      "[4,     8] loss: 0.001\n",
      "[4,     9] loss: 0.001\n",
      "[4,    10] loss: 0.001\n",
      "[4,    11] loss: 0.001\n",
      "[4,    12] loss: 0.001\n",
      "[4,    13] loss: 0.001\n",
      "[4,    14] loss: 0.001\n",
      "[4,    15] loss: 0.001\n",
      "[4,    16] loss: 0.001\n",
      "[4,    17] loss: 0.001\n",
      "[4,    18] loss: 0.001\n",
      "[4,    19] loss: 0.001\n",
      "[4,    20] loss: 0.001\n",
      "[4,    21] loss: 0.001\n",
      "[4,    22] loss: 0.001\n",
      "[4,    23] loss: 0.001\n",
      "[4,    24] loss: 0.001\n",
      "[4,    25] loss: 0.001\n",
      "[5,     1] loss: 0.001\n",
      "[5,     2] loss: 0.001\n",
      "[5,     3] loss: 0.001\n",
      "[5,     4] loss: 0.001\n",
      "[5,     5] loss: 0.001\n",
      "[5,     6] loss: 0.001\n",
      "[5,     7] loss: 0.001\n",
      "[5,     8] loss: 0.001\n",
      "[5,     9] loss: 0.001\n",
      "[5,    10] loss: 0.001\n",
      "[5,    11] loss: 0.001\n",
      "[5,    12] loss: 0.001\n",
      "[5,    13] loss: 0.001\n",
      "[5,    14] loss: 0.001\n",
      "[5,    15] loss: 0.001\n",
      "[5,    16] loss: 0.001\n",
      "[5,    17] loss: 0.001\n",
      "[5,    18] loss: 0.001\n",
      "[5,    19] loss: 0.001\n",
      "[5,    20] loss: 0.001\n",
      "[5,    21] loss: 0.001\n",
      "[5,    22] loss: 0.001\n",
      "[5,    23] loss: 0.001\n",
      "[5,    24] loss: 0.001\n",
      "[5,    25] loss: 0.001\n",
      "[6,     1] loss: 0.001\n",
      "[6,     2] loss: 0.001\n",
      "[6,     3] loss: 0.001\n",
      "[6,     4] loss: 0.001\n",
      "[6,     5] loss: 0.001\n",
      "[6,     6] loss: 0.001\n",
      "[6,     7] loss: 0.001\n",
      "[6,     8] loss: 0.001\n",
      "[6,     9] loss: 0.001\n",
      "[6,    10] loss: 0.001\n",
      "[6,    11] loss: 0.001\n",
      "[6,    12] loss: 0.001\n",
      "[6,    13] loss: 0.001\n",
      "[6,    14] loss: 0.001\n",
      "[6,    15] loss: 0.001\n",
      "[6,    16] loss: 0.001\n",
      "[6,    17] loss: 0.001\n",
      "[6,    18] loss: 0.001\n",
      "[6,    19] loss: 0.001\n",
      "[6,    20] loss: 0.001\n",
      "[6,    21] loss: 0.001\n",
      "[6,    22] loss: 0.001\n",
      "[6,    23] loss: 0.001\n",
      "[6,    24] loss: 0.001\n",
      "[6,    25] loss: 0.001\n",
      "[7,     1] loss: 0.001\n",
      "[7,     2] loss: 0.001\n",
      "[7,     3] loss: 0.001\n",
      "[7,     4] loss: 0.001\n",
      "[7,     5] loss: 0.001\n",
      "[7,     6] loss: 0.001\n",
      "[7,     7] loss: 0.001\n",
      "[7,     8] loss: 0.001\n",
      "[7,     9] loss: 0.001\n",
      "[7,    10] loss: 0.001\n",
      "[7,    11] loss: 0.001\n",
      "[7,    12] loss: 0.001\n",
      "[7,    13] loss: 0.001\n",
      "[7,    14] loss: 0.001\n",
      "[7,    15] loss: 0.001\n",
      "[7,    16] loss: 0.001\n",
      "[7,    17] loss: 0.001\n",
      "[7,    18] loss: 0.001\n",
      "[7,    19] loss: 0.001\n",
      "[7,    20] loss: 0.001\n",
      "[7,    21] loss: 0.001\n",
      "[7,    22] loss: 0.001\n",
      "[7,    23] loss: 0.001\n",
      "[7,    24] loss: 0.001\n",
      "[7,    25] loss: 0.001\n",
      "[8,     1] loss: 0.001\n",
      "[8,     2] loss: 0.001\n",
      "[8,     3] loss: 0.001\n",
      "[8,     4] loss: 0.001\n",
      "[8,     5] loss: 0.001\n",
      "[8,     6] loss: 0.001\n",
      "[8,     7] loss: 0.001\n",
      "[8,     8] loss: 0.001\n",
      "[8,     9] loss: 0.001\n",
      "[8,    10] loss: 0.001\n",
      "[8,    11] loss: 0.001\n",
      "[8,    12] loss: 0.001\n",
      "[8,    13] loss: 0.001\n",
      "[8,    14] loss: 0.001\n",
      "[8,    15] loss: 0.001\n",
      "[8,    16] loss: 0.001\n",
      "[8,    17] loss: 0.001\n",
      "[8,    18] loss: 0.001\n",
      "[8,    19] loss: 0.001\n",
      "[8,    20] loss: 0.001\n",
      "[8,    21] loss: 0.001\n",
      "[8,    22] loss: 0.001\n",
      "[8,    23] loss: 0.001\n",
      "[8,    24] loss: 0.001\n",
      "[8,    25] loss: 0.001\n",
      "[9,     1] loss: 0.001\n",
      "[9,     2] loss: 0.001\n",
      "[9,     3] loss: 0.001\n",
      "[9,     4] loss: 0.001\n",
      "[9,     5] loss: 0.001\n",
      "[9,     6] loss: 0.001\n",
      "[9,     7] loss: 0.001\n",
      "[9,     8] loss: 0.001\n",
      "[9,     9] loss: 0.001\n",
      "[9,    10] loss: 0.001\n",
      "[9,    11] loss: 0.001\n",
      "[9,    12] loss: 0.001\n",
      "[9,    13] loss: 0.001\n",
      "[9,    14] loss: 0.001\n",
      "[9,    15] loss: 0.001\n",
      "[9,    16] loss: 0.001\n",
      "[9,    17] loss: 0.001\n",
      "[9,    18] loss: 0.001\n",
      "[9,    19] loss: 0.001\n",
      "[9,    20] loss: 0.001\n",
      "[9,    21] loss: 0.001\n",
      "[9,    22] loss: 0.001\n",
      "[9,    23] loss: 0.001\n",
      "[9,    24] loss: 0.001\n",
      "[9,    25] loss: 0.001\n",
      "[10,     1] loss: 0.001\n",
      "[10,     2] loss: 0.001\n",
      "[10,     3] loss: 0.001\n",
      "[10,     4] loss: 0.001\n",
      "[10,     5] loss: 0.001\n",
      "[10,     6] loss: 0.001\n",
      "[10,     7] loss: 0.001\n",
      "[10,     8] loss: 0.001\n",
      "[10,     9] loss: 0.001\n",
      "[10,    10] loss: 0.001\n",
      "[10,    11] loss: 0.001\n",
      "[10,    12] loss: 0.001\n",
      "[10,    13] loss: 0.001\n",
      "[10,    14] loss: 0.001\n",
      "[10,    15] loss: 0.001\n",
      "[10,    16] loss: 0.001\n",
      "[10,    17] loss: 0.001\n",
      "[10,    18] loss: 0.001\n",
      "[10,    19] loss: 0.001\n",
      "[10,    20] loss: 0.001\n",
      "[10,    21] loss: 0.001\n",
      "[10,    22] loss: 0.001\n",
      "[10,    23] loss: 0.001\n",
      "[10,    24] loss: 0.001\n",
      "[10,    25] loss: 0.001\n",
      "Training Complete\n"
     ]
    }
   ],
   "source": [
    "# Setting up CUDA Device\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Available\")\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"\")\n",
    "\n",
    "net.to(device)\n",
    "\n",
    "for epoch in range(EPOCHS): # looping over dataset\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get inputs\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        \n",
    "        # zero parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass --> backward pass --> optim\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # return statistics\n",
    "        running_loss += loss.item()\n",
    "        #if i % 2000 == 1999:\n",
    "        print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "        running_loss = 0.0\n",
    "            \n",
    "print(\"Training Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "eca61f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './cifar_net_gauss.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Truth:  bird  bird  car   cat  \n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show img from test set\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('Truth: ', ' '.join(f'{testset.classes[labels[j]]:5s}' for j in range(BATCH_SIZE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "000081cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n"
     ]
    }
   ],
   "source": [
    "print(labels.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e92c6db6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted:  frog  horse car   truck\n"
     ]
    }
   ],
   "source": [
    "# loading saved model\n",
    "net = Net()\n",
    "net.load_state_dict(torch.load('./cifar_net.pth'))\n",
    "\n",
    "# Testing against above images\n",
    "outputs = net(images)\n",
    "\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join(f'{testset.classes[predicted[j]]:5s}'\n",
    "                              for j in range(BATCH_SIZE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7804ae50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 17 %\n"
     ]
    }
   ],
   "source": [
    "# Now test on entire dataset\n",
    "correct = 0 \n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        \n",
    "        # Pick class with highest similarity score\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "print(f'Accuracy of the network on the test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a645c6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class: plane is 0.0 %\n",
      "Accuracy for class: car   is 0.0 %\n",
      "Accuracy for class: bird  is 0.0 %\n",
      "Accuracy for class: cat   is 0.0 %\n",
      "Accuracy for class: deer  is 0.0 %\n",
      "Accuracy for class: dog   is 0.0 %\n",
      "Accuracy for class: frog  is 100.0 %\n",
      "Accuracy for class: horse is 0.0 %\n",
      "Accuracy for class: ship  is 0.0 %\n",
      "Accuracy for class: truck is 0.0 %\n"
     ]
    }
   ],
   "source": [
    "# Breakdown\n",
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
