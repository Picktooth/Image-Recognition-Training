{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Recognition Training\n",
    "#### Practice creating and optimizing image recognition models using the CIFAR10 data set. Current training using Kfold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Torchvision\n",
    "\"\"\"\n",
    "# Basics\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "# For CNN\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# For Loss Func. And Optimization\n",
    "import torch.optim as optim\n",
    "# Utility\n",
    "import torchvision.datasets\n",
    "from torchvision.datasets import ImageFolder as img_fold\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\"\"\"\n",
    "Numpy\n",
    "\"\"\"\n",
    "# For plots and \n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "Sklearn\n",
    "\"\"\"\n",
    "# Validation and Cross-validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Extra\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "\n",
    "print(\"Success\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 100\n",
    "WORKERS = 4\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Available\")\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"\")\n",
    "    \n",
    "# Image Categories\n",
    "classes = ('airplane', 'automobile', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck') \n",
    "\n",
    "# normalizer\n",
    "transformer = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Bringing in CIFAR10 training and test data\n",
    "\"\"\"\n",
    "CIFAR_Train = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transformer)\n",
    "CIFAR_Test = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                        download=True, transform=transformer)\n",
    "\n",
    "\"\"\"\n",
    "Bringing in custom data\n",
    "\"\"\"\n",
    "# Setting root directory for custom training and test data; Use datasets in CNN_Dataset for training\n",
    "trainroot = \"./CNN_Dataset/Level 1\"\n",
    "testroot = \"./Noiseimg/test\"\n",
    "\n",
    "# Using ImageFolder to designate and transform our entire custom dataset (determine if necessary to transform here and in dataloaders)\n",
    "trainset = img_fold(root=trainroot, transform=transformer)\n",
    "noise_testset = img_fold(root=testroot, transform=transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing tensors for images and labels in custom dataset\n",
    "# Not necessary to run if training on the CIFAR10 dataset\n",
    "img, label = trainset[0][0], trainset[0][1]\n",
    "\n",
    "# Verifying information\n",
    "print(f\"Image tensor:\\n\\n{img}\\n\")\n",
    "print(f\"Image shape: {img.shape}\")\n",
    "print(f\"Image datatype: {img.dtype}\")\n",
    "print(f\"Image label: {label}\")\n",
    "print(f\"Label datatype: {type(label)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the following code cell if you want to train on EITHER the CIFAR10 or custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE THIS IF NOT MERGING CIFAR10 AND CUSTOM DATASET\n",
    "\n",
    "# Grabbing labels from dataset\n",
    "# Modify depending on what dataset you use\n",
    "class_names = CIFAR_Train.classes\n",
    "print('Labels: ',class_names,'\\n')\n",
    "\n",
    "# Verifying length of CIFAR10 Dataset\n",
    "print('CIFAR10 Trainset Length: ', len(CIFAR_Train))\n",
    "print('CIFAR10 Testset Length: ', len(CIFAR_Test),'\\n')\n",
    "\n",
    "# Verifying length of custom dataset\n",
    "print('Custom Trainset Length: ', len(trainset))\n",
    "print('Custom Testset Length: ', len(noise_testset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the following two code cells if you want to train on BOTH the CIFAR10 and custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging CIFAR10 and custom datasets\n",
    "# ONLY USE TO IF YOU WANT TO TRAIN ON BOTH NORMAL AND NOISY IMAGES\n",
    "Merged_dataset = torch.utils.data.ConcatDataset([CIFAR_Train, trainset])\n",
    "\n",
    "# Grabbing labels from merged dataset\n",
    "class_names = Merged_dataset.datasets[0].classes\n",
    "print('Labels: ', class_names, '\\n')\n",
    "\n",
    "# Verifying information\n",
    "print(f\"Image tensor:\\n\\n{img}\\n\")\n",
    "print(f\"Image shape: {img.shape}\")\n",
    "print(f\"Image datatype: {img.dtype}\")\n",
    "print(f\"Image label: {label}\")\n",
    "print(f\"Label datatype: {type(label)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing tensors for images and labels in merged dataset\n",
    "img, label = Merged_dataset[0][0], Merged_dataset[0][1]\n",
    "\n",
    "# Verifying information\n",
    "print(f\"Image tensor:\\n\\n{img}\\n\")\n",
    "print(f\"Image shape: {img.shape}\")\n",
    "print(f\"Image datatype: {img.dtype}\")\n",
    "print(f\"Image label: {label}\")\n",
    "print(f\"Label datatype: {type(label)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Creating dataloaders\n",
    "\"\"\"\n",
    "\n",
    "# Modify depending on what dataset you want to use\n",
    "trainloader = torch.utils.data.DataLoader(Merged_dataset, batch_size=BATCH_SIZE,\n",
    "                                            shuffle=True, num_workers=4)\n",
    "\n",
    "# TestLoader for CIFAR10 images\n",
    "CIFAR10_testloader=torch.utils.data.DataLoader(CIFAR_Test, batch_size = BATCH_SIZE,\n",
    "                                            shuffle=True, num_workers=4)\n",
    "\n",
    "# TestLoader for Custom data\n",
    "Noise_testloader = torch.utils.data.DataLoader(noise_testset, batch_size=BATCH_SIZE,\n",
    "                                            shuffle=True, num_workers=4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Creating validation loaders for validation training\n",
    "\"\"\"\n",
    "\n",
    "# Split dataset into train and validation sets (ONLY NECESSARY FOR val_train_model FUNCTION)\n",
    "# Modify depending on what dataset you want to use\n",
    "train_set, val_set = train_test_split(Merged_dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "# Dataloader for validation set\n",
    "valloader = torch.utils.data.DataLoader(val_set, batch_size=4, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Showing sample images from training set\n",
    "\"\"\"\n",
    "# Defining function to show images from training set\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # Unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "    \n",
    "# Randomly select images from training set\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "# Print labels for each shown image\n",
    "print('Actual labels: ', ' '.join(f'{class_names[labels[j]]}' for j in range(BATCH_SIZE)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifying information\n",
    "print(f\"Image shape: {img.shape} -> [batch_size, color_channels, height, width]\")\n",
    "print(f\"Label shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Training Functions\n",
    "\"\"\"\n",
    "\n",
    "# Train only on entire training set\n",
    "def full_train_model(model, trainloader, criterion, optimizer, device, epochs):\n",
    "    \n",
    "    \"\"\"\n",
    "    Train the model on the training data.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model.\n",
    "        trainloader (DataLoader): DataLoader for the training dataset.\n",
    "        criterion: The loss criterion.\n",
    "        optimizer: The optimization algorithm.\n",
    "        device: The device to which the model and data should be moved (e.g., \"cuda\" or \"cpu\").\n",
    "        epochs (int): Number of epochs for training.\n",
    "    \"\"\"\n",
    "    \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "        train_accuracy = correct_train / total_train\n",
    "\n",
    "        print(f'Epoch {epoch + 1}, Training Loss: {running_loss / len(trainloader):.3f}, Training Accuracy: {100 * train_accuracy:.2f}%')\n",
    "\n",
    "    print(\"Training Complete\")\n",
    "\n",
    "# Train on a training and validation set\n",
    "def val_train_model(model, trainloader, valloader, criterion, optimizer, device, epochs):\n",
    "\n",
    "    \"\"\"\n",
    "    Train the model using train-validation technique. Make sure to split the initial training set into a training and validation set\n",
    "    and the dataloader for validation set is initialized!\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model.\n",
    "        trainloader (DataLoader): DataLoader for the training dataset.\n",
    "        valloader (DataLoader: DataLoader for the validation set)\n",
    "        criterion: The loss criterion.\n",
    "        optimizer: The optimization algorithm.\n",
    "        device: The device to which the model and data should be moved (e.g., \"cuda\" or \"cpu\").\n",
    "        epochs (int): Number of epochs for training.\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        for images, labels in trainloader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "        train_accuracy = correct_train / total_train\n",
    "\n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct_predictions = 0\n",
    "        val_total_samples = 0\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for images, labels in valloader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # calculate accuracy for validation\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total_samples += labels.size(0)\n",
    "                val_correct_predictions += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_accuracy = val_correct_predictions / val_total_samples\n",
    "    \n",
    "        print(f'Epoch {epoch + 1}, Training Loss: {running_loss / len(trainloader):.3f}, Training Accuracy: {100 * train_accuracy:.2f}%, Validation Loss: {val_loss / len(valloader)}, Validation Accuracy: {val_accuracy:.2%}')\n",
    "\n",
    "    print(\"Training Complete\")\n",
    "    \n",
    "def kfold_train_model(model, dataset, criterion, optimizer, device, k_folds, epochs, batch_size, workers, patience):\n",
    "    \n",
    "    \"\"\"\n",
    "    Train the model using kfold cross-validation technique.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model.\n",
    "        dataset: dataset for training. Will be split into train and validation sets\n",
    "        criterion: The loss criterion.\n",
    "        optimizer: The optimization algorithm.\n",
    "        device: The device to which the model and data should be moved (e.g., \"cuda\" or \"cpu\").\n",
    "        epochs (int): Number of epochs for training.\n",
    "        batch_size (int): Batch size for DataLoader.\n",
    "        workers (int): Number of workers for DataLoader.\n",
    "        patience (int): Number of epochs with no improvement after which training will be stopped.\n",
    "    \"\"\"\n",
    "    writer = SummaryWriter()\n",
    "\n",
    "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n",
    "        \n",
    "        # Data splitting\n",
    "        train_dataset = torch.utils.data.Subset(dataset, train_idx)\n",
    "        val_dataset = torch.utils.data.Subset(dataset, val_idx)\n",
    "\n",
    "        # Creating dataloaders\n",
    "        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=workers)\n",
    "        valloader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=workers)\n",
    "        \n",
    "        model.to(device)\n",
    "\n",
    "        # Move optimizer to GPU. Necessary for optimizers like Adagrad\n",
    "        for state in optimizer.state.values():\n",
    "            for k, v in state.items():\n",
    "                if torch.is_tensor(v):\n",
    "                    state[k] = v.to(device)\n",
    "                    \n",
    "        \n",
    "        # Training loop for each epoch\n",
    "        for epoch in range(epochs):  \n",
    "            running_loss = 0.0\n",
    "            correct_train = 0\n",
    "            total_train = 0\n",
    "\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            \n",
    "            best_val_loss = float('inf')\n",
    "            no_improvement_epochs = 0\n",
    "\n",
    "            \n",
    "            for i, (images, labels) in enumerate(trainloader):\n",
    "                \n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_train += labels.size(0)\n",
    "                correct_train += (predicted == labels).sum().item()\n",
    "                \n",
    "                # Print intermediate values\n",
    "                if i % 100 == 99:  # Print every 100 batches\n",
    "                    train_accuracy_batch = correct_train / total_train\n",
    "                    print(f'Fold {fold + 1}, Epoch {epoch + 1}, Batch {i + 1}, Loss: {loss.item()}, Accuracy: {100 * train_accuracy_batch:.2f}%')\n",
    "\n",
    "\n",
    "            train_accuracy = correct_train / total_train\n",
    "\n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            val_correct_predictions = 0\n",
    "            val_total_samples = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for images, labels in valloader:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    outputs = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                    # calculate accuracy for validation\n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    val_total_samples += labels.size(0)\n",
    "                    val_correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "            val_accuracy = val_correct_predictions / val_total_samples\n",
    "\n",
    "            # Print loss and accuracy per epoch for both training and validation\n",
    "            print(f'Fold {fold + 1}, Epoch {epoch + 1}, Training Loss: {running_loss / len(trainloader):.3f}, Training Accuracy: {100 * train_accuracy:.2f}%, Validation Loss: {val_loss / len(valloader)}, Validation Accuracy: {val_accuracy:.2%}')\n",
    "            \n",
    "            \n",
    "            # TensorBoard logging\n",
    "            writer.add_scalar(f'Loss/Train/Fold_{fold + 1}', running_loss / len(trainloader), epoch)\n",
    "            writer.add_scalar(f'Loss/Validation/Fold_{fold + 1}', val_loss / len(valloader), epoch)\n",
    "            writer.add_scalar(f'Accuracy/Train/Fold_{fold + 1}', 100 * train_accuracy, epoch)\n",
    "            writer.add_scalar(f'Accuracy/Validation/Fold_{fold + 1}', 100 * val_accuracy, epoch)\n",
    "            \n",
    "            # Early stopping check\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                no_improvement_epochs = 0\n",
    "            else:\n",
    "                no_improvement_epochs += 1\n",
    "\n",
    "            if no_improvement_epochs >= patience:\n",
    "                print(f'Early stopping! No improvement in validation loss for {patience} epochs.')\n",
    "                break\n",
    "            \n",
    "            print(\"\")\n",
    "\n",
    "    # Close TensorBoard writer\n",
    "    writer.close()\n",
    "    print(\"Training Complete\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test Function\n",
    "def test_model(model, model_name, testloader, device):\n",
    "    \n",
    "    \"\"\"\n",
    "    Test the model on the entire dataset and store statistics.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model.\n",
    "        model_name: Name of the neural network model. Used in storing statistics\n",
    "        testloader (DataLoader): DataLoader for the test dataset.\n",
    "        device: The device to which the model and data should be moved (e.g., \"cuda\" or \"cpu\").\n",
    "\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0 \n",
    "    total = 0\n",
    "    \n",
    "    correct_pred = {classname: 0 for classname in classes}\n",
    "    total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Pick class with highest similarity score\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    print(f'Accuracy of the network on the test images: {100 * correct // total} %\\n')\n",
    "    \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            \n",
    "            # collect the correct predictions for each class\n",
    "            for label, prediction in zip(labels, predictions):\n",
    "                if label == prediction:\n",
    "                    correct_pred[classes[label]] += 1\n",
    "                total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "    # print accuracy for each class\n",
    "    for classname, correct_count in correct_pred.items():\n",
    "        accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "        print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')\n",
    "        \n",
    "    # store statistics\n",
    "    write_accuracy_to_file(f'{model_name}_accuracy.txt', model_name, correct, total, correct_pred, total_pred, EPOCHS)\n",
    "\n",
    "# Saving statistics    \n",
    "def write_accuracy_to_file(filename, model_name, correct, total, correct_pred, total_pred, epochs):\n",
    "\n",
    "    \"\"\"\n",
    "    Store model statistics. Designed to be called within test_model function\n",
    "    \"\"\"\n",
    "    # Open the specified text file in write mode\n",
    "    with open(filename, 'w') as file:\n",
    "        # Write overall accuracy to the file\n",
    "        file.write(f'Model: {model_name}\\n\\n')\n",
    "        file.write(f'Total Epochs: {epochs}\\n\\n')\n",
    "        file.write(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %\\n\\n')\n",
    "\n",
    "        # Write accuracy for each class to the file\n",
    "        for classname, correct_count in correct_pred.items():\n",
    "            accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "            file.write(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %\\n')\n",
    "\n",
    "# Saving model (needs debugging)    \n",
    "def save_model(model, name):\n",
    "    PATH = fr\"./Models/{name}\"\n",
    "    torch.save(model, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Creating three convolutional layers and utilizing batch normalization after each convolution\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(128)\n",
    "        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.batch_norm3 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        # Dropout Layers\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "\n",
    "        # Pooling and fully connected layers\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(256 * 4 * 4, 1024)\n",
    "        self.batch_norm_fc1 = nn.BatchNorm1d(1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.batch_norm_fc2 = nn.BatchNorm1d(512)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.batch_norm_fc3 = nn.BatchNorm1d(256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.batch_norm_fc4 = nn.BatchNorm1d(128)\n",
    "        self.fc5 = nn.Linear(128, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.batch_norm1(self.conv1(x))))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.pool(F.relu(self.batch_norm2(self.conv2(x))))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.pool(F.relu(self.batch_norm3(self.conv3(x))))\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = x.view(-1, 256 * 4 * 4)\n",
    "        \n",
    "        x = F.relu(self.batch_norm_fc1(self.fc1(x)))\n",
    "        x = F.relu(self.batch_norm_fc2(self.fc2(x)))\n",
    "        x = F.relu(self.batch_norm_fc3(self.fc3(x)))\n",
    "        x = F.relu(self.batch_norm_fc4(self.fc4(x)))\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DO NOT USE! MODIFY BASED ON NEW MODELS!\n",
    "\n",
    "# Creating new models\n",
    "\n",
    "model_train_level_1 = Net()\n",
    "model_train_val_level_1 = Net()\n",
    "model_train_vc_level_1 = Net()\n",
    "\n",
    "# Copying weights of old models to retrain\n",
    "model_train_level_1.load_state_dict(torch.load('./Models/cifar_net.pth'))\n",
    "model_train_val_level_1.load_state_dict(torch.load('./Models/cifar_net_Validation.pth'))\n",
    "model_train_vc_level_1.load_state_dict(torch.load('./Models/cifar_net_kfold.pth'))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up loss func. and optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.SGD(test.parameters(), lr = 0.00001, momentum = 0.9, weight_decay=.001)\n",
    "# optimizer = optim.Adam(test.parameters(), lr = 0.0001, weight_decay=.001)\n",
    "optimizer = optim.Adagrad(test.parameters(), lr=0.001, weight_decay=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_train_model(test, trainloader, criterion, optimizer, device, EPOCHS)\n",
    "# val_train_model(model_train_val_level_1, trainloader, valloader, criterion, optimizer, device, EPOCHS)\n",
    "kfold_train_model(test, Merged_dataset, criterion, optimizer, device, 10, EPOCHS, BATCH_SIZE, 4, 2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Retrieve batch of images from test set\n",
    "\"\"\"\n",
    "\n",
    "dataiter = iter(CIFAR10_testloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show img from test set\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('Truth: ', ' '.join(f'{CIFAR_Test.classes[labels[j]]:5s}' for j in range(BATCH_SIZE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading saved model\n",
    "net = Net()\n",
    "net.load_state_dict(torch.load('./Models/Merged_Kfold_AdaGrad_F10_E100_B32_W4.pth'))\n",
    "\n",
    "# Testing against above images\n",
    "outputs = net(images)\n",
    "\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', ' '.join(f'{CIFAR_Test.classes[predicted[j]]:5s}'\n",
    "                              for j in range(BATCH_SIZE)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing model on entire test test\n",
    "test_model(net, 'CIFAR10_Merged_Kfold_AdaGrad_F10_E100_B32_W4', CIFAR10_testloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './Models/Merged_Kfold_AdaGrad_F10_E100_B32_W4.pth'\n",
    "torch.save(test.state_dict(), PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
